# Phase 4: SQL Batch Writer

**Goal:** Replace the placeholder processing in TelemetryProcessorService with real database persistence. Batch messages and bulk-insert them into SQL Server using `SqlBulkCopy`, sustaining 150+ msg/sec end-to-end.

---

## Step 1: Set Up SQL Server in Docker

### What You'll Learn
- Running SQL Server locally in Docker
- Creating a time-series optimized table schema

### Tasks
1. **Add SQL Server to your `docker-compose.yml`:**
   - Use the `mcr.microsoft.com/mssql/server:2022-latest` image
   - Expose port 1433
   - Set `SA_PASSWORD` and `ACCEPT_EULA=Y` environment variables

2. **Create the telemetry table** — connect via SSMS, Azure Data Studio, or `sqlcmd` and run:
   ```sql
   CREATE DATABASE SiteSense;
   GO
   USE SiteSense;
   GO
   CREATE TABLE TelemetryPoints (
       Id BIGINT IDENTITY(1,1) PRIMARY KEY,
       Timestamp DATETIME2(3) NOT NULL,
       VehicleId NVARCHAR(50) NOT NULL,
       Latitude FLOAT NOT NULL,
       Longitude FLOAT NOT NULL,
       Elevation FLOAT NOT NULL,
       VibrationFrequency FLOAT NOT NULL,
       CompactionValue FLOAT NOT NULL,
       IngestedAt DATETIME2(3) NOT NULL DEFAULT SYSUTCDATETIME()
   );
   ```

3. **Add a connection string** to `IngestionService/appsettings.json`:
   ```json
   {
     "ConnectionStrings": {
       "SiteSense": "Server=localhost,1433;Database=SiteSense;User Id=sa;Password=YourPassword;TrustServerCertificate=True"
     }
   }
   ```

---

## Step 2: Build the Batch Writer Class

### What You'll Learn
- `SqlBulkCopy` — .NET's high-performance bulk insert API
- `DataTable` as the in-memory buffer for bulk operations
- Column mappings between your model and the database table

### Tasks
1. **Install the NuGet package** in IngestionService:
   - `Microsoft.Data.SqlClient`

2. **Create `BatchWriter.cs`** (or `TelemetryBatchWriter.cs`):
   - Constructor takes a connection string
   - Single public method: `Task WriteBatchAsync(IReadOnlyList<TelemetryPoint> batch)`
   - Inside the method:
     a. Build a `DataTable` with columns matching the SQL table (skip `Id` and `IngestedAt` — those are auto-generated)
     b. Loop through the batch, add a row per `TelemetryPoint`
     c. Open a `SqlConnection`, create a `SqlBulkCopy` instance
     d. Set `DestinationTableName = "TelemetryPoints"`
     e. Map columns explicitly (don't rely on ordinal position)
     f. Call `await bulkCopy.WriteToServerAsync(dataTable)`

3. **Register in DI** — either as a singleton or scoped service

---

## Step 3: Add Batching Logic to TelemetryProcessorService

### What You'll Learn
- Accumulating items into a batch from a channel
- Flush triggers: batch size vs time-based flush
- Why you need both (size handles high throughput, time handles quiet periods)

### Tasks
1. **Replace the simple `await foreach` loop** with batching logic:
   - Accumulate messages into a `List<TelemetryPoint>`
   - Flush the batch when either:
     a. The batch reaches a target size (e.g., 500 items), OR
     b. A time limit expires (e.g., 1 second) — so stale data doesn't sit in memory during quiet periods

2. **Implementation approach using `ReadAsync` with a timer:**
   ```
   while not cancelled:
     try to read from channel with a timeout (e.g., 1 second)
     if got a message:
       add to batch
       if batch.Count >= 500:
         flush batch
     if timeout expired and batch has items:
       flush batch
   ```

   Hint: Use a `CancellationTokenSource.CreateLinkedTokenSource` with a timeout to implement "read with timeout."

3. **On flush:**
   - Call `batchWriter.WriteBatchAsync(batch)`
   - Log: `[Processor] Flushed batch of {count} to SQL ({elapsed}ms)`
   - Clear the batch list
   - Track total rows written

---

## Step 4: Run End-to-End with Persistence

### Tasks
1. Start Docker (Mosquitto + SQL Server)
2. Start `IngestionService`
3. Start `CompactorSimulator`
4. Watch the processor logs — you should see batches flushing to SQL
5. **Query the database** to verify data landed:
   ```sql
   SELECT COUNT(*) FROM TelemetryPoints;
   SELECT TOP 10 * FROM TelemetryPoints ORDER BY Timestamp DESC;
   ```

### Experiments
- **Tune batch size:** Try 100, 500, 1000. How does it affect throughput and queue depth?
- **Check for data loss:** Compare total messages sent (simulator) vs total rows in DB. Are they equal?
- **Kill the database:** Stop the SQL container while messages are flowing. What happens? Does the channel buffer? Do you lose data?

---

## Step 5: Add Error Handling and Resilience

### What You'll Learn
- What happens when the database is slow or unavailable
- Retry strategies for transient failures

### Tasks
1. **Wrap `WriteBatchAsync` in a try-catch:**
   - On failure, log the error and the batch size (so you know how many messages were lost)
   - Don't re-enqueue failed messages for now — just count them as lost
   - In production you'd add retry logic or a dead-letter queue, but keep it simple here

2. **Add a batch write duration metric:**
   - Time each `WriteBatchAsync` call with a `Stopwatch`
   - Log: `[Processor] Batch of 500 written in 45ms`
   - This tells you how much headroom you have

---

## Success Criteria

- [ ] SQL Server running in Docker alongside Mosquitto
- [ ] TelemetryPoints table created with appropriate schema
- [ ] BatchWriter uses SqlBulkCopy to bulk-insert data
- [ ] Processor batches messages (500 items or 1-second timeout)
- [ ] Batches flush to SQL with timing metrics logged
- [ ] Data verified in the database via SQL query
- [ ] System sustains 150+ msg/sec end-to-end with data persisted
- [ ] Errors during batch write are caught and logged (no crash)

---

## Key Concepts to Understand

1. **SqlBulkCopy vs Individual INSERTs:**
   - Individual INSERT: ~1-5ms per row (network round-trip + transaction)
   - SqlBulkCopy of 500 rows: ~20-50ms total (one round-trip, minimal logging)
   - At 150 msg/sec, individual INSERTs would need 150 round-trips/sec — possible but fragile
   - SqlBulkCopy batching needs only ~0.3 round-trips/sec (one batch every ~3 seconds)

2. **Batch Size Tradeoffs:**
   - Bigger batches = fewer round-trips = higher throughput
   - Bigger batches = more data in memory = more data lost on crash
   - Bigger batches = higher latency (data waits longer before hitting DB)
   - 500 is a common starting point — tune based on your observed write times

3. **Time-Based Flush:**
   - Without it, a batch of 3 messages could sit in memory forever during a quiet period
   - The 1-second timeout ensures data lands in the DB within ~1 second even at low throughput
   - This is a common pattern in log aggregators, metrics collectors, and event processors

4. **DataTable as a Buffer:**
   - `SqlBulkCopy` accepts `DataTable`, `IDataReader`, or `DataRow[]`
   - `DataTable` is simplest for small-to-medium batches
   - For very high throughput, implementing `IDataReader` avoids the DataTable allocation overhead

---

## Next Phase Preview

With data flowing into SQL, you could extend this project with: time-series queries and dashboards, data retention policies, horizontal scaling with multiple processor instances, or a REST API to query telemetry data.
