# Phase 3: Channel-Based Decoupling (Producer-Consumer)

**Goal:** Insert a `Channel<TelemetryPoint>` between the MQTT subscriber and a new processing service. By the end, your receiver and processor run independently — the MQTT callback stays fast, and the processor works at its own pace.

---

## Step 1: Create the Channel as a Shared Service

### What You'll Learn
- `System.Threading.Channels` — .NET's built-in, high-performance async producer-consumer queue
- Bounded vs Unbounded channels and why bounded matters
- Registering a channel as a singleton in DI

### Tasks
1. **Add the channel to DI** in `Program.cs`:
   - Create a `Channel<TelemetryPoint>` with a bounded capacity (e.g., 10,000)
   - Choose a `BoundedChannelFullMode` — start with `Wait` (backpressure: producer blocks when full)
   - Register both `ChannelWriter<TelemetryPoint>` and `ChannelReader<TelemetryPoint>` as singletons so the producer and consumer get the correct end of the pipe

2. **Why bounded?**
   - An unbounded channel grows forever if the consumer falls behind — eventually you run out of memory
   - A bounded channel with `Wait` mode applies backpressure: the MQTT handler slows down when the buffer is full
   - 10,000 capacity at 150 msg/sec gives ~66 seconds of buffer before backpressure kicks in

---

## Step 2: Modify MqttSubscriberService to Write to the Channel

### What You'll Learn
- How to inject and use `ChannelWriter<T>`
- The difference between `TryWrite` and `WriteAsync`
- Why the MQTT callback must stay non-blocking

### Tasks
1. **Inject `ChannelWriter<TelemetryPoint>`** into `MqttSubscriberService`

2. **Update the message handler:**
   - After deserializing the `TelemetryPoint`, write it to the channel
   - Use `TryWrite()` (non-blocking) rather than `await WriteAsync()` — the MQTT callback should not block
   - If `TryWrite` returns `false`, the channel is full — log a warning and increment a "dropped messages" counter

3. **Remove any processing logic** from the MQTT handler — it should only deserialize and enqueue. Keep the throughput metrics (msg/sec counter).

---

## Step 3: Create the TelemetryProcessorService

### What You'll Learn
- Reading from a `ChannelReader<T>` in a `BackgroundService`
- `await foreach` with `ReadAllAsync` — the idiomatic way to consume a channel
- How cancellation flows through the channel

### Tasks
1. **Create `TelemetryProcessorService.cs`** extending `BackgroundService`:
   - Inject `ChannelReader<TelemetryPoint>`
   - In `ExecuteAsync`, use `await foreach` with `ReadAllAsync(stoppingToken)` to consume messages
   - For now, just log every N messages (e.g., every 1000th) — real persistence comes in Phase 4

2. **Add throughput metrics** to the processor:
   - Track messages processed per second (same pattern as the subscriber)
   - Log: `[Processor] Processed 148 msg/sec (total: 12000, queue depth: 342)`

3. **Register the service** in `Program.cs`:
   ```csharp
   builder.Services.AddHostedService<TelemetryProcessorService>();
   ```

---

## Step 4: Expose Channel Depth for Observability

### What You'll Learn
- Why monitoring queue depth matters for backpressure systems
- How `Channel.Reader.Count` works (approximation, not exact)

### Tasks
1. **Log the channel's current item count** periodically from the processor:
   - `_channelReader.Count` gives an approximate pending item count
   - Include this in your per-second metrics log line

2. **Think about what the queue depth tells you:**
   - Depth near 0 → processor is keeping up easily
   - Depth growing steadily → processor is slower than producer
   - Depth at capacity (10,000) → backpressure is active, producer is being slowed

---

## Step 5: Run End-to-End and Stress Test

### Tasks
1. Start the Mosquitto broker
2. Start the `IngestionService` (now has both subscriber and processor)
3. Start the `CompactorSimulator`
4. Watch both sets of metrics — subscriber msg/sec and processor msg/sec should be roughly equal

### Experiments
- **Simulate slow processing:** Add a `Task.Delay(10)` inside the processor loop. Watch the queue depth grow. What happens when it hits 10,000?
- **Burst test:** Start 5 vehicles at 100 Hz (500 msg/sec). Does the channel absorb the burst? How high does the depth go?
- **Graceful shutdown:** Press Ctrl+C. Does the processor drain remaining messages before exiting, or does it stop immediately? (Hint: `ReadAllAsync` completes when the channel is completed AND drained)

---

## Success Criteria

- [ ] Channel registered as a bounded singleton with capacity 10,000
- [ ] MqttSubscriberService writes to channel using `TryWrite` (non-blocking)
- [ ] Dropped messages are counted and logged when channel is full
- [ ] TelemetryProcessorService reads from channel using `await foreach`
- [ ] Both services log independent throughput metrics (msg/sec)
- [ ] Queue depth is logged for observability
- [ ] System sustains 150+ msg/sec end-to-end with queue depth staying near zero
- [ ] Adding artificial delay to processor causes queue depth to grow (proves decoupling works)

---

## Key Concepts to Understand

1. **Bounded vs Unbounded Channels:**
   - `Channel.CreateBounded<T>(capacity)` — fixed buffer, supports backpressure
   - `Channel.CreateUnbounded<T>()` — infinite buffer, no backpressure, risk of OOM
   - Always prefer bounded in production systems

2. **BoundedChannelFullMode options:**
   - `Wait` — producer blocks until space is available (backpressure)
   - `DropNewest` — silently drops the newest item to make room
   - `DropOldest` — silently drops the oldest item to make room
   - `DropWrite` — silently drops the item being written
   - For telemetry, `Wait` is safest. For real-time displays, `DropOldest` might be acceptable.

3. **Why `TryWrite` instead of `WriteAsync` in the MQTT handler:**
   - `WriteAsync` is `async` and blocks until space is available — safe but stalls the MQTT client thread
   - `TryWrite` returns immediately with `true`/`false` — keeps the MQTT callback fast
   - The tradeoff: `TryWrite` drops messages when full, `WriteAsync` preserves all messages but risks stalling the receiver
   - In this design, we accept occasional drops under extreme load to keep the receiver responsive

4. **The Producer-Consumer Pattern:**
   - Producer (MQTT subscriber) and Consumer (processor) run on separate threads
   - The channel is the thread-safe handoff point — no locks needed in your code
   - This is the same pattern used by Kafka consumers, RabbitMQ workers, and Go's goroutines with channels

---

## Next Phase Preview

In Phase 4, you'll replace the placeholder processing logic with **SqlBulkCopy** batch writes. The processor will accumulate messages into batches (e.g., 500 items) and bulk-insert them into SQL Server — turning 500 individual inserts into one high-performance operation.
